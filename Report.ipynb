{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cee1a36",
   "metadata": {},
   "source": [
    "# Navigation Project report\n",
    "\n",
    "A number of different agents were created to solve the 'Banana Navition' game. These were:\n",
    "\n",
    "1. Basic Deep Queue Network using experience replay, using a linear neural network model\n",
    "2. Similar to 1, but including a 'Double' network\n",
    "3. Building on 2, but also including a 'Dueling' network\n",
    "4. Prioritised experience replay\n",
    "\n",
    "All of these implementations 'solved' the solution in so much as they achieved an average score over 100 episodes > 13. Results of these are shown below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0fd1c",
   "metadata": {},
   "source": [
    "## Learning algorithm\n",
    "Something about the learning algorithms\n",
    "\n",
    "## Plot of rewards Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb925ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name=\"Banana_Linux/Banana.x86_64\")\n",
    "#env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "\n",
    "import torch\n",
    "from lib.agents import AgentExerperienceReplay, AgentPrioritizedExperience\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.dqn import dqn\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)\n",
    "\n",
    "\n",
    "agents = []\n",
    "\n",
    "agent = AgentExerperienceReplay(state_size=state_size, action_size=action_size, seed=0, train_mode=False)\n",
    "#agent.load_model('qnetwork')\n",
    "agents.append({'agent': agent, \n",
    "               'name': 'qnetwork', \n",
    "               'test': True,\n",
    "              })\n",
    "\n",
    "agent = AgentPrioritizedExperience(state_size=state_size, action_size=action_size, seed=0,\n",
    "                                   prioritized_experience=True,\n",
    "                                   compute_weights=True,\n",
    "                                   train_mode=False)\n",
    "\n",
    "\n",
    "agents.append({'agent': agent, 'name': 'ddqn_with_prioritized_experiences_qnetwork', 'test': True})\n",
    "\n",
    "agent = AgentPrioritizedExperience(state_size=state_size, action_size=action_size, seed=0,\n",
    "                                   prioritized_experience=True,\n",
    "                                   compute_weights=True,\n",
    "                                   train_mode=False)\n",
    "\n",
    "\n",
    "agents.append({'agent': agent, 'name': 'dueling_ddqn_with_prioritized_experiences_qnetwork', 'test': True})\n",
    "\n",
    "for info in agents:\n",
    "    if info['test']:\n",
    "        print('Testing', info['name'])\n",
    "        info['agent'].load_model(info['name'])\n",
    "        info['scores'] = dqn(env,\n",
    "                             brain_name,\n",
    "                             info['agent'], \n",
    "                             train_mode=False, \n",
    "                             n_episodes=100, \n",
    "                             eps_start=0.01,\n",
    "                             checkpoint=13)\n",
    "\n",
    "\n",
    "scores = agents[1]['scores']\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "scores = agents[0]['scores']\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1d60d",
   "metadata": {},
   "source": [
    "### Basic Deep Queue Network results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116051d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = agents[0]['scores']\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4c147",
   "metadata": {},
   "source": [
    "### Double Deep Queue Network results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df281766",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = agents[1]['scores']\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13440a",
   "metadata": {},
   "source": [
    "### Dueling double deep queue network results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d135c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = agents[2]['scores']\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
